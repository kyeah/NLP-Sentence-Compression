<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE abiword PUBLIC "-//ABISOURCE//DTD AWML 1.0 Strict//EN" "http://www.abisource.com/awml.dtd">
<abiword template="false" xmlns:ct="http://www.abisource.com/changetracking.dtd" xmlns:fo="http://www.w3.org/1999/XSL/Format" xmlns:math="http://www.w3.org/1998/Math/MathML" xid-max="106" xmlns:dc="http://purl.org/dc/elements/1.1/" styles="unlocked" fileformat="1.0" xmlns:svg="http://www.w3.org/2000/svg" xmlns:awml="http://www.abisource.com/awml.dtd" xmlns="http://www.abisource.com/awml.dtd" xmlns:xlink="http://www.w3.org/1999/xlink" version="0.99.2" xml:space="preserve" props="dom-dir:ltr; document-footnote-restart-section:0; document-endnote-type:numeric; document-endnote-place-enddoc:1; document-endnote-initial:1; lang:en-US; document-endnote-restart-section:0; document-footnote-restart-page:0; document-footnote-type:numeric; document-footnote-initial:1; document-endnote-place-endsection:0">
<!-- ======================================================================== -->
<!-- This file is an AbiWord document.                                        -->
<!-- AbiWord is a free, Open Source word processor.                           -->
<!-- More information about AbiWord is available at http://www.abisource.com/ -->
<!-- You should not edit this file by hand.                                   -->
<!-- ======================================================================== -->

<metadata>
<m key="abiword.date_last_changed">Tue Apr 21 22:18:34 2015
</m>
<m key="abiword.generator">AbiWord</m>
<m key="dc.creator">Kevin Yeh</m>
<m key="dc.date">Sun Apr 19 01:59:22 2015
</m>
<m key="dc.format">application/x-abiword</m>
</metadata>
<rdf>
</rdf>
<history version="98" edit-time="95116" last-saved="1429672714" uid="8f5e25f8-b24c-11e4-82b2-eb658b15f96f">
<version id="9" started="1423700322" uid="05e84b1e-b251-11e4-82b2-eb658b15f96f" auto="0" top-xid="4"/>
<version id="60" started="1429412457" uid="735f0bc4-e6fe-11e4-8dbb-ab6801e027f5" auto="0" top-xid="22"/>
<version id="88" started="1429655191" uid="bcf1196e-e878-11e4-9b6e-b3892d0444b3" auto="0" top-xid="67"/>
<version id="98" started="1429662940" uid="429db322-e89e-11e4-9144-e770c4b9c78e" auto="0" top-xid="84"/>
</history>
<styles>
<s type="P" name="Normal" followedby="Current Settings" props="font-weight:normal; font-family:Times New Roman; margin-top:0pt; color:000000; margin-left:0pt; text-align:left; widows:2; font-style:normal; text-indent:0in; text-position:normal; font-variant:normal; bgcolor:transparent; line-height:1.0; text-decoration:none; margin-bottom:0pt; margin-right:0pt; font-size:12pt; font-stretch:normal"/>
</styles>
<pagesize pagetype="A4" orientation="portrait" width="8.267717" height="11.692913" units="in" page-scale="1.000000"/>
<section xid="3" props="page-margin-footer:0.5in; page-margin-header:0.5in">
<p style="Normal" xid="4"><c props="text-decoration:underline">Kevin Yeh										   NLP HW #4</c><c props="text-decoration:underline"></c></p>
<p style="Normal" xid="1"><c props="text-decoration:none"></c><c></c></p>
<p style="Normal" xid="2" props="text-align:left"><c props="font-weight:bold; text-decoration:none; font-size:14pt">1. Paraphrastic Sentence Compression with a Character-based Metric: </c><c props="font-weight:bold; text-decoration:none; font-size:14pt"></c></p>
<p style="Normal" xid="31" props="text-align:left"><c props="font-weight:bold; text-decoration:none; font-size:14pt">    Tightening without Deletion</c><c props="font-weight:bold; text-decoration:none; font-size:14pt"></c></p>
<p style="Normal" xid="33"><c props="font-weight:bold; text-decoration:none; font-size:14pt"></c><c></c></p>
<p style="Normal" xid="32" props="font-family:Times New Roman; font-size:12pt; color:000000; text-decoration:none; text-position:normal; font-weight:normal; font-style:normal"><c props="text-decoration:none; font-weight:normal; font-size:12pt">In this paper, Napoles et al. present a character-based substitution approach to sentence compression using bilingual parallel corpora. The motivation for this method involves the strict constraints of deletion-based approaches, which leads to the </c><c props="text-decoration:none; font-weight:normal; font-size:12pt">methodical</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"> modification of sentences and, overall, abstract and robotic results. Instead, they highlight the use of paraphrases extracted from bilingual corpora and re-ranked using a novel monolingual heuristic to provide more intuitive and naturalistic summarizations of source text, for use in cases such as document simplification, mobile text, subtitling, and micro-blog constraint fitting. At a high level, the intuition behind their methods is such that shorter words can be chosen where possible, freeing up space for more content-filled words. Their results show that combined substitution and deletion compressions preserve more meaning in the same number of characters as deletion-only compressions.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="93"><c></c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="94"><c props="text-decoration:none; font-weight:normal; font-size:12pt">In generating paraphrases, the authors extract paraphrases from bilingual parallel corpora, treating any English phrases that share a common foreign phrase as potential paraphrases of each other. To rank candidates, they propose a two-step process: candidates are first ranked using the translation model probabilities p(e|f) and p(f|e), requiring the paraphrase to be of the same syntactic type, then the candidates are re-ranked using a monolingual distributional similarity metric. This metric relies on the approximate cosine similarity scores over feature counts of the phrases and has a profound impact on the quality of the paraphrase candidate rankings. Using manual ranking of 1,000 randomly selected paraphrase sets, the addition of the monolingual filtering technique after the original translation score filter introduced a stronger positive correlation between the human rankings and the produced rankings.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="95"><c></c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="96"><c props="text-decoration:none; font-weight:normal; font-size:12pt">To tighten sentences and generate a final compressed form, Napoles et al. use a dynamic programming strategy to find the combination of non-overlapping paraphrases that minimizes a sentence’s character length; the monolingual score is not used in any type of weighted score, but can be used as a threshold to ensure a certain level of confidence in the preservation of meaning. To choose between compressions of equal or near-equal length, two metrics are used: the word-overlap score between the original and candidate sentence, and the language model score of the compressed sentence.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="97"><c></c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="98"><c props="text-decoration:none; font-weight:normal; font-size:12pt">In initial evaluations using substitution-only and deletion-only methods, the substitution method performs poorly under all cosine-similarity thresholds between 0.65 and 0.95 at a 0.10 increment. They posit that this is due to erroneous paraphrase substitution of phrases with the same syntactic category and distributional similarity, but different semantics.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="99"><c></c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="100"><c props="text-decoration:none; font-weight:normal; font-size:12pt">In light of this sub-par showing, Napoles et al. then tested the viability of their approach using manual labeling of good paraphrases produced by their model. Comparing substitution-only, substitution-and-deletion, and deletion-only compression methods within 5 characters of difference between their compressions, the methods involving substitution performed better than the deletion-only method in grammaticality and meaning statistics, indicating the potential for substitution-only methods given more advanced paraphrase acquisition and ranking methods.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="44"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="36"><c props="text-decoration:none; font-weight:bold; font-size:14pt">2. Deep, Multilingual Word Alignments using Cross-Domain Corpora</c><c props="text-decoration:none; font-weight:bold; font-size:14pt"></c></p>
<p style="Normal" xid="38"><c props="text-decoration:none; font-weight:bold; font-size:14pt"></c><c></c></p>
<p style="Normal" xid="39"><c props="text-decoration:none; font-weight:normal; font-size:12pt">As noted by the original paper, there are detrimental issues in their chosen method of candidate compression ranking. Although there may be some experimentation on the use of semantic and word-sense disambiguation techniques for filtering out unlikely paraphrases, my project will mainly investigate the combination of multiple cross-domain parallel corpora in improving paraphrastic sentence compressions.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="51"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="62"><c props="text-decoration:none; font-weight:normal; font-size:12pt">Previous work has focused on using singular bilingual corpora, namely English-German corpora, to test different sentence compression techniques and ranking metrics. However, little work has investigated the use of corpora in different languages and in multiple domains. By using deep-linking word alignments across a variety of corpora and domains, more paraphrases are likely to be found, as single-domain corpora tend towards using the same type of tone and wording. This is especially true of prominent bilingual corpora, as these corpora are often produced as a result of formal international relations and proceedings, and therefore contain more pointed and less varied language. </c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="63"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="64"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c props="text-decoration:none; font-weight:normal; font-size:12pt">By combining two or more parallel corpora using linked pairs of word alignments, cross-domain paraphrases can be extracted. This diversifies the wording of the paraphrases, which can be highly beneficial and still fitting depending on the application of compression. For example, news article summarization can benefit from less strict and formal language for posting onto social blogs and websites. In addition, combined corpora can improve the translation and paraphrasing of low-density languages with sparse amounts of parallel data. Phrases in sparse parallel data sets can be mapped into larger domains, revealing a wealth of fitting paraphrases. In addition, there is potential in attempting paraphrastic sentence </c><c props="font-weight:normal; text-decoration:none; font-size:12pt; font-style:italic">decompression</c><c props="text-decoration:none; font-weight:normal; font-size:12pt; font-style:normal">, allowing condensed sentences found on microblogging websites like Twitter and Weibo to be expanded into more standard phrasings. This has direct benefits to linguistic analyses of social media, as the vast assortment of tools traditionally used for linguistic analysis and trained on large, standardized corpora would not need to be modified to fit such a complex domain.</c></p>
<p style="Normal" xid="61"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="65"><c props="text-decoration:none; font-weight:normal; font-size:12pt">In experimenting with the idea of deep, cross-domain corpora links, different techniques will be investigated. At a high level, corpora can be combined in two different ways to extract paraphrases: in parallel, using multiple different English-to-language corpora as per previous work done in the field, and in series, using deep linked alignments to combine domains. The latter will be tested using various depth limits, languages, and language orderings. I theorize that language origins and their orderings within the system will play a large role in the success of deep, multilingual word alignments due to their varied vocabulary sizes and expressions -- some languages will have more specificity within their vocabulary, and a language may express two semantic ideas as one combined expression. Deep word alignments will therefore be reliant on the compression and decompression of their semantic expressions as they are converted from one language to the next.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="66"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="55"><c props="text-decoration:none; font-weight:normal; font-size:12pt">Many of the experimental specifics will rely on the methods presented by Napoles et al, which in turn relies on the word alignment and phrase extraction methods presented in Koehn et al (2003). A newer word alignment tool like Giza++ or the Berkeley Aligner will likely be used, but I expect to use a rudimentary implementation of Koehn’s phrase extraction already available on the web. Once phrase extraction is performed on all chosen pairs of corpora, these phrase mappings can be used to link these alignments. The same ranking metrics will be used, with some possible additional experimentation in using WordNet semantic information to improve these rankings, if enough time is available after the main experiments.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="74"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="75"><c props="text-decoration:none; font-weight:normal; font-size:12pt">In terms of the specific corpora that will be used, combinations of parliamentary parallel corpora (e.g. Europarl) and more literary or informal corpora are likely candidates. Recent parallel corpora created for microblogging sites such as Twitter and Weibo have emerged in the past two years, but due to their strict publishing policies, each sentence must be mined locally. Due to this constraint, about 50% of the available corpora is now unavailable due to deleted posts, and it is extremely difficult to mine existing posts due to API request limits. Therefore, experiments involving microblogging corpora will not be possible in the time available.</c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="68"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c></c></p>
<p style="Normal" xid="69"><c props="text-decoration:none; font-weight:normal; font-size:12pt">As already noted, evaluation will include experimentation into corpora combination types (parallel vs. series), different language combinations, language order, depth limits, source/target corpora sizes, and ranking metrics. It will also include a baseline implementation of the methods presented in Napoles et al. The original paper uses manual crowd-sourced judging from Amazon Turk to rank grammar and meaning scores; since I do not have enough time, I will perform manual judging with about 200 sentences. I will also experiment with using probabilistic tools (e.g. the Stanford parser) already used or mentioned in lecture to get rough wide-scale scores. </c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="101"><c></c><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="102"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c><c props="text-decoration:none; font-weight:normal; font-size:12pt">The goal of this paper will not necessarily be to improve on the best-performing sentence compression methods, but to improve on the best-performing paraphrastic sentence compressions, which has been shown in Napoles et al to have the potential to perform as well or better than current best-performing deletion-only compressions.</c></p>
<p style="Normal" xid="78"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="84"><c props="text-decoration:none; font-weight:normal; font-size:12pt"></c></p>
<p style="Normal" xid="79" props="font-family:Times New Roman; font-size:12pt; color:000000; text-decoration:none; text-position:normal; font-weight:normal; font-style:normal"><c props="font-family:Times New Roman; text-decoration:none; color:000000; font-size:14pt; text-position:normal; font-weight:bold; font-style:normal; lang:en-US">3. References</c></p>
<p style="Normal" xid="83" props="font-family:Times New Roman; font-size:12pt; color:000000; text-decoration:none; text-position:normal; font-weight:normal; font-style:normal"><c props="font-family:Times New Roman; text-decoration:none; color:000000; font-size:14pt; text-position:normal; font-weight:bold; font-style:normal; lang:en-US"></c></p>
<p style="Normal" xid="80" props="font-family:Times New Roman; dom-dir:ltr; font-style:normal; margin-left:0.0000in; lang:en-US; margin-bottom:0.0000in; text-indent:0.0000in; text-position:normal; margin-top:0.0000in; font-weight:normal; margin-right:0.0000in; text-decoration:none; text-align:left; line-height:1.000000; font-size:12pt"><c props="font-family:Times New Roman; font-size:12pt; lang:en-US; text-position:normal; font-weight:normal; font-style:normal; text-decoration:none">Courtney Napoles, Chris Callison-Burch, Juri Ganitkevitch, and Benjamin Van Durme. 2011.</c></p>
<p style="Normal" xid="81" props="font-family:Times New Roman; dom-dir:ltr; font-style:normal; margin-left:0.0000in; lang:en-US; margin-bottom:0.0000in; text-indent:0.0000in; text-position:normal; margin-top:0.0000in; font-weight:normal; margin-right:0.0000in; text-decoration:none; text-align:left; line-height:1.000000; font-size:12pt"><c props="font-family:Times New Roman; font-size:12pt; lang:en-US; text-position:normal; font-weight:normal; font-style:normal; text-decoration:none">Paraphrastic sentence compression with a character-based metric: Tightening without deletion.</c></p>
<p style="Normal" xid="82" props="font-family:Times New Roman; dom-dir:ltr; font-style:normal; margin-left:0.0000in; lang:en-US; margin-bottom:0.0000in; text-indent:0.0000in; text-position:normal; margin-top:0.0000in; font-weight:normal; margin-right:0.0000in; text-decoration:none; text-align:left; line-height:1.000000; font-size:12pt"><c props="font-family:Times New Roman; font-size:12pt; lang:en-US; text-position:normal; font-weight:normal; font-style:normal; text-decoration:none">In Proceedings of ACL, Workshop on Monolingual Text-To-Text Generation.</c></p>
<p style="Normal" xid="103" props="font-family:Times New Roman; dom-dir:ltr; font-style:normal; margin-left:0.0000in; lang:en-US; margin-bottom:0.0000in; text-indent:0.0000in; text-position:normal; margin-top:0.0000in; font-weight:normal; margin-right:0.0000in; text-decoration:none; text-align:left; line-height:1.000000; font-size:12pt"><c props="font-family:Times New Roman; font-size:12pt; lang:en-US; text-position:normal; font-weight:normal; font-style:normal; text-decoration:none"></c></p>
<p style="Normal" xid="104" props="font-family:Times New Roman; dom-dir:ltr; font-style:normal; margin-left:0.0000in; lang:en-US; text-decoration:none; text-indent:0.0000in; text-position:normal; margin-top:0.0000in; font-weight:normal; line-height:1.000000; margin-bottom:0.0000in; font-size:12pt; margin-right:0.0000in; text-align:left"><c>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL.</c></p>
</section>
</abiword>
